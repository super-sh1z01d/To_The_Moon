#!/usr/bin/env python3
"""
Test script for token processing monitoring system.

This script tests the token monitoring functionality including:
- Status transition tracking
- Performance metrics collection
- Stuck token analysis
- Processing rate calculation
"""

import sys
import os
import asyncio
from pathlib import Path
from datetime import datetime, timedelta

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from monitoring.token_monitor import TokenProcessingMonitor, get_token_monitor


async def test_token_monitoring():
    """Test the token processing monitoring system."""
    print("ü™ô Testing Token Processing Monitoring")
    print("=" * 50)
    
    # Initialize token monitor
    token_monitor = get_token_monitor()
    
    print("üìä Token Monitor initialized")
    print()
    
    # Test 1: Record status transitions
    print("1Ô∏è‚É£ Testing status transition recording...")
    
    # Simulate some token transitions
    test_transitions = [
        ("token1", "new", "monitoring", 5.2, "initial_processing"),
        ("token2", "new", "monitoring", 3.8, "initial_processing"),
        ("token1", "monitoring", "active", 125.5, "activation_successful"),
        ("token3", "new", "monitoring", 4.1, "initial_processing"),
        ("token4", "new", "monitoring", 6.3, "initial_processing"),
        ("token2", "monitoring", "active", 89.2, "activation_successful"),
    ]
    
    for mint, from_status, to_status, proc_time, reason in test_transitions:
        token_monitor.record_status_transition(
            mint_address=mint,
            from_status=from_status,
            to_status=to_status,
            processing_time_seconds=proc_time,
            reason=reason
        )
    
    print(f"   Recorded {len(test_transitions)} transitions")
    print(f"   Transition history size: {len(token_monitor.transition_history)}")
    print()
    
    # Test 2: Calculate processing rates
    print("2Ô∏è‚É£ Testing processing rate calculations...")
    
    processing_rate = token_monitor._calculate_processing_rate()
    activation_rate = token_monitor._calculate_activation_rate()
    avg_activation_time = token_monitor._calculate_avg_activation_time()
    
    print(f"   Processing rate: {processing_rate:.2f} transitions/min")
    print(f"   Activation rate: {activation_rate:.1f} activations/hour")
    print(f"   Avg activation time: {avg_activation_time:.1f} minutes" if avg_activation_time else "   Avg activation time: N/A")
    print()
    
    # Test 3: Stuck tokens tracking
    print("3Ô∏è‚É£ Testing stuck tokens tracking...")
    
    # Add some stuck tokens to tracker
    base_time = datetime.utcnow() - timedelta(minutes=5)
    token_monitor.stuck_tokens_tracker["stuck_token_1"] = base_time
    token_monitor.stuck_tokens_tracker["stuck_token_2"] = base_time - timedelta(minutes=2)
    
    stuck_count = token_monitor._get_stuck_tokens_count()
    print(f"   Stuck tokens (>3m): {stuck_count}")
    print(f"   Tracked stuck tokens: {len(token_monitor.stuck_tokens_tracker)}")
    print()
    
    # Test 4: Activation success rate
    print("4Ô∏è‚É£ Testing activation success rate...")
    
    success_rate = token_monitor._calculate_activation_success_rate()
    print(f"   Activation success rate: {success_rate:.1f}%")
    print()
    
    # Test 5: Collect current metrics (will fail without DB, but test the structure)
    print("5Ô∏è‚É£ Testing metrics collection...")
    
    try:
        metrics = token_monitor.collect_current_metrics()
        print(f"   ‚úÖ Metrics collected successfully")
        print(f"   Monitoring tokens: {metrics.monitoring_count}")
        print(f"   Active tokens: {metrics.active_count}")
        print(f"   Processing rate: {metrics.tokens_processed_per_minute:.2f}/min")
        print(f"   Activation rate: {metrics.activations_per_hour:.1f}/hour")
        print(f"   Stuck tokens: {metrics.tokens_stuck_over_2h}")
    except Exception as e:
        print(f"   ‚ö†Ô∏è Metrics collection failed (expected without DB): {str(e)[:100]}")
    print()
    
    # Test 6: Performance summary
    print("6Ô∏è‚É£ Testing performance summary...")
    
    try:
        summary = token_monitor.get_performance_summary()
        print(f"   ‚úÖ Performance summary generated")
        print(f"   Current metrics keys: {list(summary['current_metrics'].keys())}")
        print(f"   Trends available: {'trends' in summary and bool(summary['trends'])}")
        print(f"   Issues tracked: {len(summary['issues'])}")
        print(f"   History size: {summary['transition_history_size']}")
    except Exception as e:
        print(f"   ‚ö†Ô∏è Performance summary failed: {str(e)[:100]}")
    print()
    
    # Test 7: Analyze stuck tokens (will fail without DB, but test structure)
    print("7Ô∏è‚É£ Testing stuck token analysis...")
    
    try:
        stuck_analysis = token_monitor.analyze_stuck_tokens(limit=5)
        print(f"   ‚úÖ Stuck token analysis completed")
        print(f"   Analyzed tokens: {len(stuck_analysis)}")
        
        for analysis in stuck_analysis[:2]:  # Show first 2
            print(f"   - {analysis.mint_address}: {analysis.time_in_monitoring_hours:.1f}h in monitoring")
            print(f"     Blocking conditions: {analysis.blocking_conditions}")
            
    except Exception as e:
        print(f"   ‚ö†Ô∏è Stuck token analysis failed (expected without DB): {str(e)[:100]}")
    print()
    
    # Test 8: Transition history analysis
    print("8Ô∏è‚É£ Testing transition history analysis...")
    
    # Analyze recent transitions
    recent_transitions = list(token_monitor.transition_history)[-5:]
    print(f"   Recent transitions: {len(recent_transitions)}")
    
    for transition in recent_transitions:
        print(f"   - {transition.mint_address}: {transition.from_status} ‚Üí {transition.to_status}")
        if transition.processing_time_seconds:
            print(f"     Processing time: {transition.processing_time_seconds:.1f}s")
    print()
    
    # Test 9: Metrics history
    print("9Ô∏è‚É£ Testing metrics history...")
    
    print(f"   Metrics history size: {len(token_monitor.metrics_history)}")
    print(f"   Processing rate samples: {len(token_monitor.processing_rate_samples)}")
    print()
    
    print("‚úÖ Token monitoring tests completed!")
    return True


if __name__ == "__main__":
    try:
        result = asyncio.run(test_token_monitoring())
        if result:
            print("\nüéâ All token monitoring tests passed!")
            sys.exit(0)
        else:
            print("\n‚ùå Some token monitoring tests failed!")
            sys.exit(1)
    except Exception as e:
        print(f"\nüí• Test failed with error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)